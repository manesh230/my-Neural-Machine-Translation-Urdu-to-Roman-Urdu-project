{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4b0WOmtZLRD",
        "outputId": "fce8e0f7-95f4-4325-86f8-478950f54c6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'urdu_ghazals_rekhta'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 112 (delta 7), reused 6 (delta 6), pack-reused 103 (from 1)\u001b[K\n",
            "Receiving objects: 100% (112/112), 2.03 MiB | 20.22 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/amir9ume/urdu_ghazals_rekhta.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya8S6_bCZQYY"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/urdu_ghazals_rekhta/dataset/dataset.zip -d /content/urdu_ghazals_rekhta/dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7L6VyJyZe7K",
        "outputId": "ebceda40-e270-44e6-e709-6c76fd2e7297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: یِہ  كہاں  هے؟\n",
            "After : یہ کہاں هے\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# path to dataset\n",
        "base_path = \"/content/urdu_ghazals_rekhta/dataset/dataset\"\n",
        "\n",
        "def normalize_urdu(text):\n",
        "    # Normalize common Urdu characters\n",
        "    text = re.sub(\"[يى]\", \"ی\", text)   # different forms of 'yeh'\n",
        "    text = re.sub(\"[ۀہۂھ]\", \"ہ\", text)  # standardize 'heh'\n",
        "    text = re.sub(\"[ك]\", \"ک\", text)     # Arabic kaaf -> Urdu kaaf\n",
        "    text = re.sub(\"[ۃ]\", \"ہ\", text)\n",
        "\n",
        "    # Remove diacritics\n",
        "    text = re.sub(r\"[\\u064B-\\u0652]\", \"\", text)\n",
        "\n",
        "    # Remove punctuation (optional: keep , ? ! if useful for modeling)\n",
        "    text = re.sub(r\"[^\\w\\sءاآأإا-ی]\", \" \", text)\n",
        "\n",
        "    # Collapse multiple spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test\n",
        "sample = \"یِہ  كہاں  هے؟\"   # intentionally messy\n",
        "print(\"Before:\", sample)\n",
        "print(\"After :\", normalize_urdu(sample))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsgqZjRQZnIa",
        "outputId": "024e8286-cfda-49e2-fe0d-02e958661981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total pairs: 21003\n",
            "Example Urdu  : خمار موسم خوشبو حد چمن میں کہلا\n",
            "Example Roman : ḳhumār-e-mausam-e-ḳhushbū had-e-chaman meñ khulā\n"
          ]
        }
      ],
      "source": [
        "urdu_sentences = []\n",
        "roman_sentences = []\n",
        "\n",
        "for poet in os.listdir(base_path):\n",
        "    poet_path = os.path.join(base_path, poet)\n",
        "    urdu_path = os.path.join(poet_path, \"ur\")\n",
        "    eng_path  = os.path.join(poet_path, \"en\")\n",
        "\n",
        "    if not os.path.exists(urdu_path) or not os.path.exists(eng_path):\n",
        "        continue\n",
        "\n",
        "    # match files by name in urdu and eng\n",
        "    for fname in os.listdir(urdu_path):\n",
        "        urdu_file = os.path.join(urdu_path, fname)\n",
        "        eng_file  = os.path.join(eng_path, fname)\n",
        "\n",
        "        if not os.path.exists(eng_file):\n",
        "            continue\n",
        "\n",
        "        with open(urdu_file, \"r\", encoding=\"utf-8\") as f1, open(eng_file, \"r\", encoding=\"utf-8\") as f2:\n",
        "            urdu_lines = f1.readlines()\n",
        "            eng_lines  = f2.readlines()\n",
        "\n",
        "            for u, e in zip(urdu_lines, eng_lines):\n",
        "                u = normalize_urdu(u.strip())\n",
        "                e = e.strip().lower()   # roman target → lowercase\n",
        "\n",
        "                if u and e:\n",
        "                    urdu_sentences.append(u)\n",
        "                    roman_sentences.append(e)\n",
        "\n",
        "print(\"Total pairs:\", len(urdu_sentences))\n",
        "print(\"Example Urdu  :\", urdu_sentences[0])\n",
        "print(\"Example Roman :\", roman_sentences[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX29ArMTZr2b",
        "outputId": "4e9b67ee-875c-41cc-ce28-9704934d7b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                    urdu  \\\n",
            "0        خمار موسم خوشبو حد چمن میں کہلا   \n",
            "1      مری غزل کا خزانہ ترے بدن میں کہلا   \n",
            "2  تم اس کا حسن کبہی اس کی بزم میں دیکہو   \n",
            "3     کہ ماہتاب سدا شب کے پیرہن میں کہلا   \n",
            "4      عجب نشہ تہا مگر اس کی بخشش لب میں   \n",
            "\n",
            "                                              roman  \n",
            "0  ḳhumār-e-mausam-e-ḳhushbū had-e-chaman meñ khulā  \n",
            "1       mirī ġhazal kā ḳhazāna tire badan meñ khulā  \n",
            "2         tum us kā husn kabhī us kī bazm meñ dekho  \n",
            "3         ki māhtāb sadā shab ke pairahan meñ khulā  \n",
            "4    ajab nasha thā magar us kī baḳhshish-e-lab meñ  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\"urdu\": urdu_sentences, \"roman\": roman_sentences})\n",
        "df.to_csv(\"/content/urdu_roman_dataset.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zYkvpWOZvo1",
        "outputId": "3ebc55ac-751a-4e17-96b5-fae8cbe7a9b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Urdu tokens : ['▁خمار', '▁موسم', '▁خوشبو', '▁حد', '▁چمن', '▁میں', '▁کہلا']\n",
            "Roman tokens: ['▁ḳhumār', '-', 'e', '-', 'mausam', '-', 'e', '-', 'ḳhushbū', '▁had', '-', 'e', '-', 'chaman', '▁meñ', '▁khulā']\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Write Urdu text to file for training tokenizer\n",
        "with open(\"urdu.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in urdu_sentences:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Write Roman Urdu text\n",
        "with open(\"roman.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in roman_sentences:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "# Train BPE model for Urdu (vocab size ~8000)\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"urdu.txt\",\n",
        "    model_prefix=\"urdu_bpe\",\n",
        "    vocab_size=8000,\n",
        "    character_coverage=0.995,  # covers almost all Urdu chars\n",
        "    model_type=\"bpe\"\n",
        ")\n",
        "\n",
        "# Train BPE model for Roman Urdu (vocab size ~8000)\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=\"roman.txt\",\n",
        "    model_prefix=\"roman_bpe\",\n",
        "    vocab_size=8000,\n",
        "    character_coverage=1.0,\n",
        "    model_type=\"bpe\"\n",
        ")\n",
        "\n",
        "# Load trained tokenizers\n",
        "sp_urdu = spm.SentencePieceProcessor(model_file=\"urdu_bpe.model\")\n",
        "sp_roman = spm.SentencePieceProcessor(model_file=\"roman_bpe.model\")\n",
        "\n",
        "# Example encoding & decoding\n",
        "sample_urdu = urdu_sentences[0]\n",
        "sample_roman = roman_sentences[0]\n",
        "\n",
        "print(\"Urdu tokens :\", sp_urdu.encode(sample_urdu, out_type=str))\n",
        "print(\"Roman tokens:\", sp_roman.encode(sample_roman, out_type=str))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR7dzYMpZy9z",
        "outputId": "3df6b583-92fa-48e4-e3fb-ef225eb44752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Experiment: {'name': 'A_emb128_h256', 'vocab_size': 8000, 'emb_dim': 128, 'enc_hidden': 256, 'dec_hidden': 256, 'enc_layers': 2, 'dec_layers': 4, 'dropout': 0.3, 'lr': 0.001, 'batch_size': 64, 'epochs': 30, 'patience': 5, 'tf': 0.5, 'max_len': 120} ===\n",
            "urdu_bpe.model found, loading existing.\n",
            "roman_bpe.model found, loading existing.\n",
            "Total: 21003, train: 10501, val: 5250, test: 5252\n",
            "vocab sizes (src,tgt): 8000 8000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | train_loss 6.3151 | val_loss 6.0435 | time 7.4s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | train_loss 5.9728 | val_loss 6.0192 | time 5.5s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | train_loss 5.8010 | val_loss 5.6085 | time 6.0s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | train_loss 5.4038 | val_loss 5.3301 | time 5.5s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | train_loss 5.1379 | val_loss 5.1567 | time 6.0s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | train_loss 4.9380 | val_loss 5.0205 | time 6.2s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | train_loss 4.7460 | val_loss 4.8623 | time 6.0s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | train_loss 4.5602 | val_loss 4.7408 | time 5.5s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | train_loss 4.3919 | val_loss 4.6085 | time 6.1s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | train_loss 4.2182 | val_loss 4.5134 | time 5.5s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | train_loss 4.0480 | val_loss 4.4004 | time 6.0s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | train_loss 3.8805 | val_loss 4.3200 | time 5.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 | train_loss 3.7347 | val_loss 4.2801 | time 5.8s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 | train_loss 3.5966 | val_loss 4.1709 | time 5.8s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 | train_loss 3.4704 | val_loss 4.1290 | time 5.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 | train_loss 3.3422 | val_loss 4.0632 | time 5.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 | train_loss 3.2303 | val_loss 4.0409 | time 5.5s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 | train_loss 3.1229 | val_loss 3.9994 | time 6.2s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 | train_loss 3.0093 | val_loss 3.9951 | time 5.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 | train_loss 2.9236 | val_loss 3.9743 | time 6.1s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 | train_loss 2.8199 | val_loss 3.9598 | time 5.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 | train_loss 2.7212 | val_loss 3.9557 | time 6.2s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23 | train_loss 2.6378 | val_loss 3.9539 | time 5.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24 | train_loss 2.5466 | val_loss 3.9368 | time 6.2s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25 | train_loss 2.4621 | val_loss 3.9367 | time 5.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26 | train_loss 2.3843 | val_loss 3.9408 | time 6.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27 | train_loss 2.3050 | val_loss 3.9315 | time 5.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28 | train_loss 2.2291 | val_loss 3.9463 | time 6.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29 | train_loss 2.1562 | val_loss 3.9509 | time 5.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30 | train_loss 2.0862 | val_loss 3.9588 | time 6.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final test loss: 3.887688068022211\n",
            "\n",
            "=== Experiment: {'name': 'B_emb256_h512', 'vocab_size': 8000, 'emb_dim': 256, 'enc_hidden': 512, 'dec_hidden': 512, 'enc_layers': 2, 'dec_layers': 4, 'dropout': 0.3, 'lr': 0.0005, 'batch_size': 64, 'epochs': 30, 'patience': 6, 'tf': 0.5, 'max_len': 120} ===\n",
            "urdu_bpe.model found, loading existing.\n",
            "roman_bpe.model found, loading existing.\n",
            "Total: 21003, train: 10501, val: 5250, test: 5252\n",
            "vocab sizes (src,tgt): 8000 8000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | train_loss 6.3002 | val_loss 6.0367 | time 11.4s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | train_loss 5.9536 | val_loss 6.0005 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | train_loss 5.8680 | val_loss 5.8845 | time 11.7s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | train_loss 5.5140 | val_loss 5.3994 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | train_loss 5.0623 | val_loss 5.0294 | time 11.7s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | train_loss 4.6965 | val_loss 4.7319 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | train_loss 4.3803 | val_loss 4.5236 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | train_loss 4.1107 | val_loss 4.3619 | time 11.5s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | train_loss 3.8636 | val_loss 4.2137 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | train_loss 3.6454 | val_loss 4.0877 | time 11.5s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | train_loss 3.4389 | val_loss 4.0346 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | train_loss 3.2521 | val_loss 3.9068 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 | train_loss 3.0678 | val_loss 3.8688 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 | train_loss 2.9006 | val_loss 3.8062 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 | train_loss 2.7409 | val_loss 3.7637 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 | train_loss 2.5908 | val_loss 3.7584 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 | train_loss 2.4433 | val_loss 3.7511 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 | train_loss 2.3055 | val_loss 3.7022 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 | train_loss 2.1682 | val_loss 3.7066 | time 11.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 | train_loss 2.0358 | val_loss 3.6888 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 | train_loss 1.9153 | val_loss 3.7123 | time 11.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 | train_loss 1.7973 | val_loss 3.6841 | time 11.6s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23 | train_loss 1.6844 | val_loss 3.7020 | time 11.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24 | train_loss 1.5739 | val_loss 3.7117 | time 11.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25 | train_loss 1.4759 | val_loss 3.7552 | time 11.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26 | train_loss 1.3776 | val_loss 3.7170 | time 11.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27 | train_loss 1.2837 | val_loss 3.7401 | time 11.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28 | train_loss 1.1953 | val_loss 3.7658 | time 11.6s\n",
            "Early stopping triggered. Stopping training.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final test loss: 3.683548941669694\n",
            "\n",
            "=== Experiment: {'name': 'C_emb256_h512_drop50', 'vocab_size': 8000, 'emb_dim': 256, 'enc_hidden': 512, 'dec_hidden': 512, 'enc_layers': 2, 'dec_layers': 4, 'dropout': 0.5, 'lr': 0.0001, 'batch_size': 32, 'epochs': 40, 'patience': 8, 'tf': 0.5, 'max_len': 120} ===\n",
            "urdu_bpe.model found, loading existing.\n",
            "roman_bpe.model found, loading existing.\n",
            "Total: 21003, train: 10501, val: 5250, test: 5252\n",
            "vocab sizes (src,tgt): 8000 8000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | train_loss 6.4366 | val_loss 6.0297 | time 14.8s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | train_loss 5.8621 | val_loss 5.7346 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | train_loss 5.5375 | val_loss 5.4437 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | train_loss 5.2900 | val_loss 5.2589 | time 15.0s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | train_loss 5.0928 | val_loss 5.0656 | time 15.0s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | train_loss 4.9011 | val_loss 4.9034 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | train_loss 4.7326 | val_loss 4.7581 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | train_loss 4.5840 | val_loss 4.6461 | time 14.8s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | train_loss 4.4507 | val_loss 4.5513 | time 15.2s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | train_loss 4.3338 | val_loss 4.4574 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | train_loss 4.2232 | val_loss 4.3874 | time 14.8s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | train_loss 4.1180 | val_loss 4.3234 | time 14.8s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 | train_loss 4.0215 | val_loss 4.2600 | time 15.1s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 | train_loss 3.9262 | val_loss 4.2055 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 | train_loss 3.8414 | val_loss 4.1494 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 | train_loss 3.7562 | val_loss 4.1092 | time 14.8s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 | train_loss 3.6715 | val_loss 4.0717 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 | train_loss 3.5931 | val_loss 4.0287 | time 15.2s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 | train_loss 3.5128 | val_loss 3.9784 | time 14.8s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 | train_loss 3.4421 | val_loss 3.9384 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 | train_loss 3.3671 | val_loss 3.9213 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 | train_loss 3.2942 | val_loss 3.8758 | time 15.2s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23 | train_loss 3.2244 | val_loss 3.8427 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24 | train_loss 3.1571 | val_loss 3.8228 | time 14.8s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25 | train_loss 3.0897 | val_loss 3.8050 | time 14.8s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26 | train_loss 3.0202 | val_loss 3.7719 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27 | train_loss 2.9603 | val_loss 3.7469 | time 15.2s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28 | train_loss 2.8958 | val_loss 3.7169 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29 | train_loss 2.8361 | val_loss 3.7077 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30 | train_loss 2.7793 | val_loss 3.6893 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31 | train_loss 2.7169 | val_loss 3.6734 | time 15.3s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32 | train_loss 2.6617 | val_loss 3.6558 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33 | train_loss 2.6066 | val_loss 3.6463 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34 | train_loss 2.5463 | val_loss 3.6244 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35 | train_loss 2.4921 | val_loss 3.6271 | time 15.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36 | train_loss 2.4437 | val_loss 3.6007 | time 15.1s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37 | train_loss 2.3881 | val_loss 3.5962 | time 15.0s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38 | train_loss 2.3359 | val_loss 3.5983 | time 14.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39 | train_loss 2.2843 | val_loss 3.5956 | time 14.9s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40 | train_loss 2.2327 | val_loss 3.5849 | time 15.2s\n",
            "  Saved best model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final test loss: 3.51837947585366\n",
            "All experiment results: [('A_emb128_h256', {'train_loss': 2.0861891428629558, 'val_loss': 3.9588143595729965, 'test_loss': 3.887688068022211}), ('B_emb256_h512', {'train_loss': 1.1952744223854759, 'val_loss': 3.7657759160880584, 'test_loss': 3.683548941669694}), ('C_emb256_h512_drop50', {'train_loss': 2.232714864258346, 'val_loss': 3.5848672606728296, 'test_loss': 3.51837947585366})]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# Colab prerequisites\n",
        "# !pip install sentencepiece torch torchvision --quiet\n",
        "\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# ========== Config (change as needed) ==========\n",
        "CSV_PATH = \"/content/urdu_roman_dataset.csv\"   # your CSV\n",
        "URDU_MODEL = \"urdu_bpe.model\"   # or train and point to these\n",
        "ROMAN_MODEL = \"roman_bpe.model\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# ========== Utility: load or train SentencePiece ==========\n",
        "# If you already have urdu_bpe.model and roman_bpe.model, skip training.\n",
        "def train_sp_if_missing(sentences, model_prefix, vocab_size=8000, model_type=\"bpe\", character_coverage=1.0):\n",
        "    if os.path.exists(model_prefix + \".model\"):\n",
        "        print(f\"{model_prefix}.model found, loading existing.\")\n",
        "        sp = spm.SentencePieceProcessor(model_file=model_prefix + \".model\")\n",
        "        return sp\n",
        "    # create tmp file for sentencepiece training\n",
        "    tmp_txt = model_prefix + \".txt\"\n",
        "    with open(tmp_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "        for s in sentences:\n",
        "            f.write(s + \"\\n\")\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=tmp_txt,\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=character_coverage,\n",
        "        model_type=model_type,\n",
        "        pad_id=0, unk_id=1, bos_id=2, eos_id=3  # set reserved ids\n",
        "    )\n",
        "    sp = spm.SentencePieceProcessor(model_file=model_prefix + \".model\")\n",
        "    os.remove(tmp_txt)\n",
        "    return sp\n",
        "\n",
        "# ========== Dataset + tokenizer wrapper ==========\n",
        "class ParallelDataset(Dataset):\n",
        "    def __init__(self, urdu_texts, roman_texts, sp_urdu, sp_roman, max_len=200):\n",
        "        assert len(urdu_texts) == len(roman_texts)\n",
        "        self.urdu = urdu_texts\n",
        "        self.roman = roman_texts\n",
        "        self.sp_urdu = sp_urdu\n",
        "        self.sp_roman = sp_roman\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # use SentencePiece's built-in IDs for special tokens (as we set in trainer)\n",
        "        # pad_id = 0, unk_id = 1, bos_id = 2, eos_id = 3 (if trained with these)\n",
        "        self.pad_id_urdu = 0\n",
        "        self.pad_id_roman = 0\n",
        "        self.bos_id_roman = 2\n",
        "        self.eos_id_roman = 3\n",
        "        self.bos_id_urdu = 2\n",
        "        self.eos_id_urdu = 3\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.urdu)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = self.urdu[idx]\n",
        "        tgt = self.roman[idx]\n",
        "\n",
        "        # encode as IDs (SentencePiece returns list of ids)\n",
        "        src_ids = self.sp_urdu.encode(src, out_type=int)\n",
        "        tgt_ids = self.sp_roman.encode(tgt, out_type=int)\n",
        "\n",
        "        # add bos/eos to target (decoder requires sos/bos)\n",
        "        tgt_input = [self.bos_id_roman] + tgt_ids\n",
        "        tgt_output = tgt_ids + [self.eos_id_roman]\n",
        "\n",
        "        # clip long sequences\n",
        "        if len(src_ids) > self.max_len:\n",
        "            src_ids = src_ids[:self.max_len]\n",
        "        if len(tgt_input) > self.max_len:\n",
        "            tgt_input = tgt_input[:self.max_len]\n",
        "            tgt_output = tgt_output[:self.max_len]\n",
        "\n",
        "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_input, dtype=torch.long), torch.tensor(tgt_output, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # batch items: (src_ids, tgt_input, tgt_output)\n",
        "    srcs, t_inps, t_outs = zip(*batch)\n",
        "    src_lens = [s.size(0) for s in srcs]\n",
        "    # pad sequences\n",
        "    srcs_padded = pad_sequence(srcs, batch_first=True, padding_value=0)  # pad_id assumed 0\n",
        "    t_inps_padded = pad_sequence(t_inps, batch_first=True, padding_value=0)\n",
        "    t_outs_padded = pad_sequence(t_outs, batch_first=True, padding_value=0)\n",
        "    return srcs_padded, torch.tensor(src_lens), t_inps_padded, t_outs_padded\n",
        "\n",
        "# ========== Model Components ==========\n",
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, enc_hidden, num_layers=2, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.LSTM(input_size=emb_dim,\n",
        "                           hidden_size=enc_hidden,\n",
        "                           num_layers=num_layers,\n",
        "                           batch_first=True,\n",
        "                           bidirectional=True,\n",
        "                           dropout=dropout if num_layers>1 else 0.0)\n",
        "        self.num_layers = num_layers\n",
        "        self.enc_hidden = enc_hidden\n",
        "        self.bidirectional = True\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # src: (B, T)\n",
        "        embedded = self.dropout(self.embedding(src))  # (B, T, E)\n",
        "        # pack\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_out, (h_n, c_n) = self.rnn(packed)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B, T, hidden*2)\n",
        "        # h_n shape: (num_layers*2, B, enc_hidden)\n",
        "        return out, (h_n, c_n)\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, dec_hidden, num_layers=4, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.LSTM(input_size=emb_dim,\n",
        "                           hidden_size=dec_hidden,\n",
        "                           num_layers=num_layers,\n",
        "                           batch_first=True,\n",
        "                           dropout=dropout if num_layers>1 else 0.0)\n",
        "        self.out = nn.Linear(dec_hidden, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.num_layers = num_layers\n",
        "        self.dec_hidden = dec_hidden\n",
        "\n",
        "    def forward(self, tgt_input, hidden):\n",
        "        # tgt_input: (B, T)  -- token ids including BOS at start\n",
        "        emb = self.dropout(self.embedding(tgt_input))  # (B, T, E)\n",
        "        outputs, (h_n, c_n) = self.rnn(emb, hidden)  # outputs: (B, T, dec_hidden)\n",
        "        logits = self.out(outputs)  # (B, T, V)\n",
        "        return logits, (h_n, c_n)\n",
        "\n",
        "# Helper to map encoder final hidden -> decoder initial hidden\n",
        "class EncToDecInit(nn.Module):\n",
        "    def __init__(self, enc_layers, enc_hidden, dec_layers, dec_hidden, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.enc_layers = enc_layers\n",
        "        self.dec_layers = dec_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.enc_hidden = enc_hidden\n",
        "        self.dec_hidden = dec_hidden\n",
        "\n",
        "        # encoder hidden has shape (num_layers*directions, B, enc_hidden)\n",
        "        enc_total_layers = enc_layers * (2 if bidirectional else 1)\n",
        "        # We'll flatten enc hidden (for each layer) and project per-decoder-layer\n",
        "        # Create linear to transform concatenated forward+back states into decoder hidden size\n",
        "        self.h_proj = nn.Linear(enc_hidden * (2 if bidirectional else 1), dec_hidden)\n",
        "        self.c_proj = nn.Linear(enc_hidden * (2 if bidirectional else 1), dec_hidden)\n",
        "\n",
        "    def forward(self, h_enc, c_enc):\n",
        "        # h_enc: (enc_total_layers, B, enc_hidden)\n",
        "        enc_total_layers = h_enc.size(0)\n",
        "        B = h_enc.size(1)\n",
        "        # We'll create decoder initial hidden by taking the top `enc_layers` pairs and projecting them\n",
        "        # Simple strategy: for each encoder-layer index i (0..enc_layers-1) combine forward+back (if bidir)\n",
        "        # and produce a vector for each decoder layer (repeat/cycle if decoder has more layers)\n",
        "        # First fold forward/back into single vector per encoder layer\n",
        "        if self.bidirectional:\n",
        "            # h_enc shape (enc_layers*2, B, H). Pair them (0,1), (2,3), ...\n",
        "            h_pairs = []\n",
        "            c_pairs = []\n",
        "            for i in range(0, enc_total_layers, 2):\n",
        "                h_f = h_enc[i]   # (B, H)\n",
        "                h_b = h_enc[i+1]\n",
        "                h_cat = torch.cat([h_f, h_b], dim=-1)  # (B, 2H)\n",
        "                h_pairs.append(h_cat)\n",
        "\n",
        "                c_f = c_enc[i]\n",
        "                c_b = c_enc[i+1]\n",
        "                c_cat = torch.cat([c_f, c_b], dim=-1)\n",
        "                c_pairs.append(c_cat)\n",
        "            # h_pairs is list len enc_layers, each (B, 2H)\n",
        "            h_pairs = torch.stack(h_pairs, dim=0)\n",
        "            c_pairs = torch.stack(c_pairs, dim=0)\n",
        "        else:\n",
        "            h_pairs = h_enc\n",
        "            c_pairs = c_enc\n",
        "\n",
        "        # Now project each encoder-layer pair to decoder hidden size\n",
        "        # We'll generate dec_layers outputs by repeating cycle if needed\n",
        "        h_dec_list = []\n",
        "        c_dec_list = []\n",
        "        for i in range(self.dec_layers):\n",
        "            src_layer = i % h_pairs.size(0)  # cycle through encoder layers\n",
        "            h_src = h_pairs[src_layer]  # (B, 2H) or (B, H)\n",
        "            c_src = c_pairs[src_layer]\n",
        "            h_proj = torch.tanh(self.h_proj(h_src))  # (B, dec_hidden)\n",
        "            c_proj = torch.tanh(self.c_proj(c_src))\n",
        "            h_dec_list.append(h_proj.unsqueeze(0))\n",
        "            c_dec_list.append(c_proj.unsqueeze(0))\n",
        "        h_dec = torch.cat(h_dec_list, dim=0)  # (dec_layers, B, dec_hidden)\n",
        "        c_dec = torch.cat(c_dec_list, dim=0)\n",
        "        return (h_dec.contiguous(), c_dec.contiguous())\n",
        "\n",
        "# ========== Training / Eval Routines ==========\n",
        "def train_epoch(encoder, dec_init, decoder, dataloader, optimizer, criterion, teacher_forcing_ratio=0.5):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    dec_init.train()\n",
        "    total_loss = 0.0\n",
        "    for srcs, src_lens, t_inps, t_outs in tqdm(dataloader, leave=False):\n",
        "        srcs = srcs.to(DEVICE)\n",
        "        src_lens = src_lens.to(DEVICE)\n",
        "        t_inps = t_inps.to(DEVICE)\n",
        "        t_outs = t_outs.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        enc_outputs, (h_n, c_n) = encoder(srcs, src_lens)  # we don't use attention, just initial state\n",
        "\n",
        "        # map to decoder initial hidden\n",
        "        h0, c0 = dec_init(h_n, c_n)\n",
        "\n",
        "        # forward decoder in teacher-forcing manner\n",
        "        logits, _ = decoder(t_inps, (h0, c0))  # returns (B, T, V)\n",
        "        B, T, V = logits.size()\n",
        "        logits_flat = logits.view(-1, V)\n",
        "        t_outs_flat = t_outs.view(-1)\n",
        "\n",
        "        loss = criterion(logits_flat, t_outs_flat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(list(encoder.parameters()) + list(decoder.parameters()) + list(dec_init.parameters()), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(encoder, dec_init, decoder, dataloader, criterion):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    dec_init.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for srcs, src_lens, t_inps, t_outs in tqdm(dataloader, leave=False):\n",
        "            srcs = srcs.to(DEVICE)\n",
        "            src_lens = src_lens.to(DEVICE)\n",
        "            t_inps = t_inps.to(DEVICE)\n",
        "            t_outs = t_outs.to(DEVICE)\n",
        "\n",
        "            enc_outputs, (h_n, c_n) = encoder(srcs, src_lens)\n",
        "            h0, c0 = dec_init(h_n, c_n)\n",
        "            logits, _ = decoder(t_inps, (h0, c0))\n",
        "            B, T, V = logits.size()\n",
        "            loss = criterion(logits.view(-1, V), t_outs.view(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Early stopping helper\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "\n",
        "    def step(self, metric):  # metric: validation loss (lower is better)\n",
        "        score = -metric\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            return False\n",
        "        if score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                return True\n",
        "            return False\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "            return False\n",
        "\n",
        "# ========== Full pipeline: load CSV, split, create dataloaders ==========\n",
        "def load_and_prepare(csv_path, sp_urdu, sp_roman, split_seed=SEED):\n",
        "    df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
        "    # expected columns: 'urdu' and 'roman'\n",
        "    urdu_texts = df['urdu'].astype(str).tolist()\n",
        "    roman_texts = df['roman'].astype(str).tolist()\n",
        "    # Shuffle and split 50/25/25\n",
        "    combined = list(zip(urdu_texts, roman_texts))\n",
        "    random.Random(split_seed).shuffle(combined)\n",
        "    n = len(combined)\n",
        "    n_train = int(0.50 * n)\n",
        "    n_val = int(0.25 * n)\n",
        "    train_set = combined[:n_train]\n",
        "    val_set = combined[n_train:n_train+n_val]\n",
        "    test_set = combined[n_train+n_val:]\n",
        "    print(f\"Total: {n}, train: {len(train_set)}, val: {len(val_set)}, test: {len(test_set)}\")\n",
        "\n",
        "    train_urdu, train_roman = zip(*train_set)\n",
        "    val_urdu, val_roman = zip(*val_set)\n",
        "    test_urdu, test_roman = zip(*test_set)\n",
        "\n",
        "    return (list(train_urdu), list(train_roman)), (list(val_urdu), list(val_roman)), (list(test_urdu), list(test_roman))\n",
        "\n",
        "# ========== Experiment / Runner ==========\n",
        "def run_experiment(config):\n",
        "    print(\"\\n=== Experiment:\", config, \"===\")\n",
        "    # Load SP models (or train if not present)\n",
        "    # Provide sentences only if models missing; here we train if missing\n",
        "    df = pd.read_csv(CSV_PATH, encoding=\"utf-8\")\n",
        "    urdu_texts = df['urdu'].astype(str).tolist()\n",
        "    roman_texts = df['roman'].astype(str).tolist()\n",
        "\n",
        "    sp_urdu = train_sp_if_missing(urdu_texts, \"urdu_bpe\", vocab_size=config['vocab_size'], character_coverage=0.995)\n",
        "    sp_roman = train_sp_if_missing(roman_texts, \"roman_bpe\", vocab_size=config['vocab_size'], character_coverage=1.0)\n",
        "\n",
        "    # prepare datasets\n",
        "    (train_urdu, train_roman), (val_urdu, val_roman), (test_urdu, test_roman) = load_and_prepare(CSV_PATH, sp_urdu, sp_roman)\n",
        "    train_ds = ParallelDataset(train_urdu, train_roman, sp_urdu, sp_roman, max_len=config['max_len'])\n",
        "    val_ds   = ParallelDataset(val_urdu, val_roman, sp_urdu, sp_roman, max_len=config['max_len'])\n",
        "    test_ds  = ParallelDataset(test_urdu, test_roman, sp_urdu, sp_roman, max_len=config['max_len'])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # vocab sizes from sentencepiece processor\n",
        "    src_vocab = sp_urdu.get_piece_size()\n",
        "    tgt_vocab = sp_roman.get_piece_size()\n",
        "    print(\"vocab sizes (src,tgt):\", src_vocab, tgt_vocab)\n",
        "\n",
        "    # build models\n",
        "    encoder = EncoderBiLSTM(vocab_size=src_vocab, emb_dim=config['emb_dim'], enc_hidden=config['enc_hidden'],\n",
        "                            num_layers=config['enc_layers'], dropout=config['dropout']).to(DEVICE)\n",
        "    decoder = DecoderLSTM(vocab_size=tgt_vocab, emb_dim=config['emb_dim'], dec_hidden=config['dec_hidden'],\n",
        "                          num_layers=config['dec_layers'], dropout=config['dropout']).to(DEVICE)\n",
        "\n",
        "    dec_init = EncToDecInit(enc_layers=config['enc_layers'], enc_hidden=config['enc_hidden'],\n",
        "                            dec_layers=config['dec_layers'], dec_hidden=config['dec_hidden']).to(DEVICE)\n",
        "\n",
        "    # criterion & optimizer\n",
        "    pad_idx = 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()) + list(dec_init.parameters()), lr=config['lr'])\n",
        "\n",
        "    # training loop\n",
        "    best_val_loss = float(\"inf\")\n",
        "    early_stopper = EarlyStopping(patience=config['patience'], delta=0.0001)\n",
        "    for epoch in range(1, config['epochs']+1):\n",
        "        t0 = time.time()\n",
        "        train_loss = train_epoch(encoder, dec_init, decoder, train_loader, optimizer, criterion, teacher_forcing_ratio=config['tf'])\n",
        "        val_loss = evaluate(encoder, dec_init, decoder, val_loader, criterion)\n",
        "        t1 = time.time()\n",
        "        print(f\"Epoch {epoch} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | time {t1-t0:.1f}s\")\n",
        "\n",
        "        # checkpoint best\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'encoder': encoder.state_dict(),\n",
        "                'decoder': decoder.state_dict(),\n",
        "                'dec_init': dec_init.state_dict(),\n",
        "                'config': config\n",
        "            }, f\"best_model_exp_{config['name']}.pt\")\n",
        "            print(\"  Saved best model.\")\n",
        "\n",
        "        # early stopping\n",
        "        if early_stopper.step(val_loss):\n",
        "            print(\"Early stopping triggered. Stopping training.\")\n",
        "            break\n",
        "\n",
        "    # final evaluation on test\n",
        "    test_loss = evaluate(encoder, dec_init, decoder, test_loader, criterion)\n",
        "    print(\"Final test loss:\", test_loss)\n",
        "    return {'train_loss': train_loss, 'val_loss': val_loss, 'test_loss': test_loss}\n",
        "\n",
        "# ========== Example experiments (modify per your assignment) ==========\n",
        "if __name__ == \"__main__\":\n",
        "    # pick three experiments (you must report & compare)\n",
        "    experiments = [\n",
        "        {   # Exp A: moderate embedding, medium hidden\n",
        "            'name': 'A_emb128_h256',\n",
        "            'vocab_size': 8000,\n",
        "            'emb_dim': 128,\n",
        "            'enc_hidden': 256,\n",
        "            'dec_hidden': 256,\n",
        "            'enc_layers': 2,    # as required in assignment\n",
        "            'dec_layers': 4,    # as required\n",
        "            'dropout': 0.3,\n",
        "            'lr': 1e-3,\n",
        "            'batch_size': 64,\n",
        "            'epochs': 30,\n",
        "            'patience': 5,\n",
        "            'tf': 0.5,  # teacher forcing ratio\n",
        "            'max_len': 120\n",
        "        },\n",
        "        {   # Exp B: larger embedding, larger hidden, smaller lr\n",
        "            'name': 'B_emb256_h512',\n",
        "            'vocab_size': 8000,\n",
        "            'emb_dim': 256,\n",
        "            'enc_hidden': 512,\n",
        "            'dec_hidden': 512,\n",
        "            'enc_layers': 2,\n",
        "            'dec_layers': 4,\n",
        "            'dropout': 0.3,\n",
        "            'lr': 5e-4,\n",
        "            'batch_size': 64,\n",
        "            'epochs': 30,\n",
        "            'patience': 6,\n",
        "            'tf': 0.5,\n",
        "            'max_len': 120\n",
        "        },\n",
        "        {   # Exp C: smaller lr, higher dropout\n",
        "            'name': 'C_emb256_h512_drop50',\n",
        "            'vocab_size': 8000,\n",
        "            'emb_dim': 256,\n",
        "            'enc_hidden': 512,\n",
        "            'dec_hidden': 512,\n",
        "            'enc_layers': 2,\n",
        "            'dec_layers': 4,\n",
        "            'dropout': 0.5,\n",
        "            'lr': 1e-4,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 40,\n",
        "            'patience': 8,\n",
        "            'tf': 0.5,\n",
        "            'max_len': 120\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    for cfg in experiments:\n",
        "        res = run_experiment(cfg)\n",
        "        results.append((cfg['name'], res))\n",
        "    print(\"All experiment results:\", results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIUNbk29Z4aT",
        "outputId": "cb6e27d8-fed1-449f-d847-d0ddbf7ce961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Roman Urdu: hazāroñ sī ik k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1. Load trained models\n",
        "checkpoint = torch.load(\"best_model_exp_A_emb128_h256.pt\", map_location=DEVICE)\n",
        "config = checkpoint[\"config\"]\n",
        "\n",
        "src_vocab = checkpoint[\"config\"][\"vocab_size\"]\n",
        "tgt_vocab = checkpoint[\"config\"][\"vocab_size\"]\n",
        "\n",
        "encoder = EncoderBiLSTM(vocab_size=sp_urdu.get_piece_size(),\n",
        "                        emb_dim=config['emb_dim'],\n",
        "                        enc_hidden=config['enc_hidden'],\n",
        "                        num_layers=config['enc_layers'],\n",
        "                        dropout=config['dropout']).to(DEVICE)\n",
        "\n",
        "decoder = DecoderLSTM(vocab_size=sp_roman.get_piece_size(),\n",
        "                      emb_dim=config['emb_dim'],\n",
        "                      dec_hidden=config['dec_hidden'],\n",
        "                      num_layers=config['dec_layers'],\n",
        "                      dropout=config['dropout']).to(DEVICE)\n",
        "\n",
        "dec_init = EncToDecInit(enc_layers=config['enc_layers'],\n",
        "                        enc_hidden=config['enc_hidden'],\n",
        "                        dec_layers=config['dec_layers'],\n",
        "                        dec_hidden=config['dec_hidden']).to(DEVICE)\n",
        "\n",
        "encoder.load_state_dict(checkpoint['encoder'])\n",
        "decoder.load_state_dict(checkpoint['decoder'])\n",
        "dec_init.load_state_dict(checkpoint['dec_init'])\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "dec_init.eval()\n",
        "\n",
        "# 2. Predict function\n",
        "def translate_sentence(sentence, sp_urdu, sp_roman, encoder, dec_init, decoder, max_len=120):\n",
        "    # encode input\n",
        "    src_ids = sp_urdu.encode(sentence, out_type=int)\n",
        "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "    src_len = torch.tensor([len(src_ids)]).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, (h_n, c_n) = encoder(src_tensor, src_len)\n",
        "        h0, c0 = dec_init(h_n, c_n)\n",
        "\n",
        "    # start with <bos>\n",
        "    bos_id = sp_roman.bos_id()\n",
        "    eos_id = sp_roman.eos_id()\n",
        "    tgt_ids = [bos_id]\n",
        "    hidden = (h0, c0)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        inp = torch.tensor([tgt_ids], dtype=torch.long).to(DEVICE)\n",
        "        logits, hidden = decoder(inp, hidden)\n",
        "        next_token = logits[:, -1, :].argmax(dim=-1).item()\n",
        "        if next_token == eos_id:\n",
        "            break\n",
        "        tgt_ids.append(next_token)\n",
        "\n",
        "    # decode ids back to text\n",
        "    return sp_roman.decode(tgt_ids[1:])  # remove BOS\n",
        "\n",
        "# 3. Example\n",
        "urdu_sentence = \"ہزاروں خواہشیں ایسی کہ ہر خواہش پہ دم نکلے\"\n",
        "print(\"Predicted Roman Urdu:\", translate_sentence(urdu_sentence, sp_urdu, sp_roman, encoder, dec_init, decoder))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyHFwSy_gC7a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Word-level tokenizer (use num_words to limit vocab size)\n",
        "tokenizer_urdu = Tokenizer(num_words=10000, oov_token=\"<unk>\")\n",
        "tokenizer_roman = Tokenizer(num_words=10000, oov_token=\"<unk>\")\n",
        "\n",
        "tokenizer_urdu.fit_on_texts(urdu_sentences)\n",
        "tokenizer_roman.fit_on_texts(roman_sentences)\n",
        "\n",
        "# Convert to sequences\n",
        "input_sequences = tokenizer_urdu.texts_to_sequences(urdu_sentences)\n",
        "target_sequences = tokenizer_roman.texts_to_sequences(roman_sentences)\n",
        "\n",
        "# Pad\n",
        "max_len = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding=\"post\")\n",
        "target_sequences = pad_sequences(target_sequences, maxlen=max_len, padding=\"post\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0PKjLCYdhWSA",
        "outputId": "e6b6f52c-e1bf-4acc-a9ac-85c83510b80c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}